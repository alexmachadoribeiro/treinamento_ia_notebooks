{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09a25a0",
   "metadata": {},
   "source": [
    "## Text mining\n",
    "\n",
    "Na aula de hoje falaremos sobre modelos de texto e técnicas para trabalhar com strings como emails, transcrições, artigos, SMSs e afins. Estas técnicas também são comumente chamadas de _text mining_.\n",
    "\n",
    "De uma forma geral, a maior parte do desafio nesse assunto é como transformar o texto em dados numéricos, isto é, fazer um _feature engineering_ que faça sentido para o problema.\n",
    "\n",
    "Existem diversas maneiras de tratar os problemas de texto em diferentes níveis de profundidade. Este processo de transformar o texto em coordenadas em um espaço vetorial é comumente chamado de _vetorização_ e estabelece relações matemáticas entre o texto, possibilitando realizar operações aritméticas com palavras. Na aula de hoje exploraremos algumas técnicas mais simples e tradicionais para ilustrar tratamentos e aplicações comuns às outras técnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9334e09",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit\n",
    "\n",
    "Usaremos bastante a biblioteca NLTK - _Natural Language Toolkit_ (https://www.nltk.org/). Esta biblioteca nos fornece uma miríade de ferramentas úteis para trabalhar com texto, além de _corpora_ de textos em várias línguas.\n",
    "\n",
    "Vamos utilizar um dataset de texto nesta aula e ir explorando as ferramentas necessárias conforme construímos os modelos. O dataset utilizado será o _Sentiment Labelled Sentences_, que possui comentários/avaliações de sites divididos entre sentimentos positivos e negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd71419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Carregando o dataset (IMBD)\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='\t', names=['comment', 'target_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c9fe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  target_sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...                 0\n",
       "1  Not sure who was more lost - the flat characte...                 0\n",
       "2  Attempting artiness with black & white and cle...                 0\n",
       "3       Very little music or anything to speak of.                   0\n",
       "4  The best scene in the movie was when Gerardo i...                 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe705e",
   "metadata": {},
   "source": [
    "### Tratamentos\n",
    "\n",
    "Vamos aplicar alguns tratamentos no texto para construirmos as variáveis. Vamos construir algumas funções para limpá-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b17c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo letras maiúsculas\n",
    "df['comment'] = df['comment'].apply(lambda s: s.lower())\n",
    "\n",
    "# Removendo pontuação\n",
    "def remove_punct(s):\n",
    "    return ''.join([c for c in s if c not in ('.', ',', '-', \"'\", '\"', '!', '?')])\n",
    "df['comment'] = df['comment'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd9876",
   "metadata": {},
   "source": [
    "Vamos usar a lib `unidecode` para remover outros caracteres especiais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4744c659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\danihell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\danihell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\danihell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ad0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo outros caracteres especiais\n",
    "from unidecode import unidecode\n",
    "df['comment'] = df['comment'].apply(unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3354a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando &\n",
    "df['comment'] = df['comment'].apply(lambda s: s.replace('&', 'and'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2d3732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very very very slowmoving aimless movie abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost  the flat character...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black and white and c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very little music or anything to speak of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  target_sentiment\n",
       "0  a very very very slowmoving aimless movie abou...                 0\n",
       "1  not sure who was more lost  the flat character...                 0\n",
       "2  attempting artiness with black and white and c...                 0\n",
       "3        very little music or anything to speak of                   0\n",
       "4  the best scene in the movie was when gerardo i...                 1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606aaa5",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "Uma das abordagens mais simples e intuitivas para modelar problemas de texto é uma contagem de frequência de palavras. Quando fazemos isto estamos ignorando a _ordem_ das palavras, mas para vários problemas isso é suficiente.\n",
    "\n",
    "Este módulo do `sklearn` faz automaticamente muitos tratamentos no texto (como a padronização em caracteres minúsculos), mas é importante estar atento a particularidades da língua utilizada, como acentos e flexões específicas.\n",
    "\n",
    "Vamos construir uma _bag of words_ com nosso dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b20ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "countvec.fit(df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7c2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_matrix = countvec.fit_transform(df['comment'])\n",
    "words_df = pd.DataFrame(words_matrix.todense(), columns=countvec.get_feature_names_out())\n",
    "df = pd.concat([df, words_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8452d53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target_sentiment</th>\n",
       "      <th>10</th>\n",
       "      <th>110</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>18th</th>\n",
       "      <th>1928</th>\n",
       "      <th>1947</th>\n",
       "      <th>1948</th>\n",
       "      <th>...</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yun</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiestudents</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very very very slowmoving aimless movie abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost  the flat character...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black and white and c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  target_sentiment  10  \\\n",
       "0  a very very very slowmoving aimless movie abou...                 0   0   \n",
       "1  not sure who was more lost  the flat character...                 0   0   \n",
       "2  attempting artiness with black and white and c...                 0   0   \n",
       "\n",
       "   110  12  15  18th  1928  1947  1948  ...  youre  yourself  youthful  \\\n",
       "0    0   0   0     0     0     0     0  ...      0         0         0   \n",
       "1    0   0   0     0     0     0     0  ...      0         0         0   \n",
       "2    0   0   0     0     0     0     0  ...      0         0         0   \n",
       "\n",
       "   youtube  youve  yun  zillion  zombie  zombiestudents  zombiez  \n",
       "0        0      0    0        0       0               0        0  \n",
       "1        0      0    0        0       0               0        0  \n",
       "2        0      0    0        0       0               0        0  \n",
       "\n",
       "[3 rows x 3147 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa106200",
   "metadata": {},
   "source": [
    "Agora temos um possível problema: existem muitas palavras no dataset, o que acarreta em uma dimensionalidade altíssima!\n",
    "\n",
    "É conhecido dentro do estudo da linguagem natural que nem toda palavra de uma língua carrega a mesma quantidade de informação - conectivos como \"ou\" e \"e\", por exemplo, carregam menos informação do que palavras como \"carro\", \"ruim\", \"ótimo\". Dessa forma, podemos remover do dataset palavras de pouca informação. Estas palavras neste contexto são chamadas de _stopwords_, e a biblioteca NLTK pode nos fornecê-las:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a151c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Danihell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Carregando stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e58d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_english[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c021e",
   "metadata": {},
   "source": [
    "Temos alguns caracteres especiais nas palavras. Vamos removê-los e tirar duplicidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de8d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = list(set([s.replace(\"'\", '') for s in stopwords_english]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e446ab1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['through', 'yourself', 'youre', 'them', 'at', 'how', 'havent', 'itself']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_english[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3625c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recarregando o dataset\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='\t', names=['comment', 'target_sentiment'])\n",
    "# Removendo letras maiúsculas\n",
    "df['comment'] = df['comment'].apply(lambda s: s.lower())\n",
    "# Removendo pontuação\n",
    "df['comment'] = df['comment'].apply(remove_punct)\n",
    "# Removendo outros caracteres especiais\n",
    "df['comment'] = df['comment'].apply(unidecode)\n",
    "# Tratando &\n",
    "df['comment'] = df['comment'].apply(lambda s: s.replace('&', 'and'))\n",
    "\n",
    "# Removendo stopwords\n",
    "def remove_stopwords(s):\n",
    "    global stopwords_english\n",
    "    token_list = s.split(' ')\n",
    "    token_list = [s for s in token_list if s not in stopwords_english]\n",
    "    return ' '.join(token_list)\n",
    "df['comment'] = df['comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b80361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slowmoving aimless movie distressed drifting y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sure lost  flat characters audience nearly hal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness black white clever camera ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>little music anything speak</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best scene movie gerardo trying find song keep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  target_sentiment\n",
       "0  slowmoving aimless movie distressed drifting y...                 0\n",
       "1  sure lost  flat characters audience nearly hal...                 0\n",
       "2  attempting artiness black white clever camera ...                 0\n",
       "3                      little music anything speak                   0\n",
       "4  best scene movie gerardo trying find song keep...                 1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a5cdd",
   "metadata": {},
   "source": [
    "Ótimo! Conseguimos remover muitas palavras do dataset. Vamos remover também os números:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68feab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo números\n",
    "def remove_nums(s):\n",
    "    return ''.join([c for c in s if c not in ('1234567890')])\n",
    "df['comment'] = df['comment'].apply(remove_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7e01497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "words_matrix = countvec.fit_transform(df['comment'])\n",
    "words_df = pd.DataFrame(words_matrix.todense(), columns=countvec.get_feature_names_out())\n",
    "df = pd.concat([df, words_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7ecc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target_sentiment</th>\n",
       "      <th>aailiyah</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>about</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstruse</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>...</th>\n",
       "      <th>youdo</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yun</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiestudents</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slowmoving aimless movie distressed drifting y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sure lost  flat characters audience nearly hal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness black white clever camera ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>little music anything speak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best scene movie gerardo trying find song keep...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  target_sentiment  \\\n",
       "0  slowmoving aimless movie distressed drifting y...                 0   \n",
       "1  sure lost  flat characters audience nearly hal...                 0   \n",
       "2  attempting artiness black white clever camera ...                 0   \n",
       "3                      little music anything speak                   0   \n",
       "4  best scene movie gerardo trying find song keep...                 1   \n",
       "\n",
       "   aailiyah  abandoned  ability  about  abroad  absolutely  abstruse  abysmal  \\\n",
       "0         0          0        0      0       0           0         0        0   \n",
       "1         0          0        0      0       0           0         0        0   \n",
       "2         0          0        0      0       0           0         0        0   \n",
       "3         0          0        0      0       0           0         0        0   \n",
       "4         0          0        0      0       0           0         0        0   \n",
       "\n",
       "   ...  youdo  young  younger  youthful  youtube  yun  zillion  zombie  \\\n",
       "0  ...      0      1        0         0        0    0        0       0   \n",
       "1  ...      0      0        0         0        0    0        0       0   \n",
       "2  ...      0      0        0         0        0    0        0       0   \n",
       "3  ...      0      0        0         0        0    0        0       0   \n",
       "4  ...      0      0        0         0        0    0        0       0   \n",
       "\n",
       "   zombiestudents  zombiez  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "3               0        0  \n",
       "4               0        0  \n",
       "\n",
       "[5 rows x 3024 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922478a",
   "metadata": {},
   "source": [
    "Este é um problema _esparso_ - o dataset possui **muitos** zeros. Isto pode representar um problema dependendo do algoritmo utilizado. Vamos ajustar uma Random Forest para termos um baseline de comparação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d4afeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=1312)\n",
    "model_cols = [c for c in df.columns if (c!='comment' and c!='target_sentiment')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a0496d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=10,\n",
       "                       min_samples_split=10, n_estimators=10, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=10,\n",
       "                       min_samples_split=10, n_estimators=10, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=10,\n",
       "                       min_samples_split=10, n_estimators=10, n_jobs=-1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfr = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=10,\n",
    "                             class_weight='balanced',\n",
    "                             min_samples_split=10,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "rfr.fit(df_train[model_cols], df_train['target_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "237368f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC treino: 0.8479692082111437\n",
      "ROC-AUC teste: 0.7828354670459934\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('ROC-AUC treino:', roc_auc_score(df_train['target_sentiment'], rfr.predict_proba(df_train[model_cols])[:,1]))\n",
    "print('ROC-AUC teste:', roc_auc_score(df_test['target_sentiment'], rfr.predict_proba(df_test[model_cols])[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77bb5e",
   "metadata": {},
   "source": [
    "Conseguimos uma discriminação boa entre as classes, mas podemos melhorá-la considerando uma propriedade simples da linguagem natural: o significado muda pouco em relação a flexões. A versão feminina de uma palavra, por exemplo, carrega praticamente a mesma informação da versão masculina. Podemos aplicar uma \"redução\" nas palavras para chegar em uma \"forma elementar\". Este processo é comumente chamado de _stemming_ e faz parte da _tokenização_ do dataset, isto é, separar em palavras (ou n-gramas) relevantes. Vamos aplicar esta técnica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "407360d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment\n",
      "practic\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Exemplo de funcionamento\n",
    "print(stemmer.stem('comments'))\n",
    "print(stemmer.stem('practically'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "927af726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recarregando o dataset\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='\t', names=['comment', 'target_sentiment'])\n",
    "# Removendo letras maiúsculas\n",
    "df['comment'] = df['comment'].apply(lambda s: s.lower())\n",
    "# Removendo pontuação\n",
    "df['comment'] = df['comment'].apply(remove_punct)\n",
    "# Removendo outros caracteres especiais\n",
    "df['comment'] = df['comment'].apply(unidecode)\n",
    "# Tratando &\n",
    "df['comment'] = df['comment'].apply(lambda s: s.replace('&', 'and'))\n",
    "\n",
    "# Removendo stopwords\n",
    "def remove_stopwords(s):\n",
    "    global stopwords_english\n",
    "    token_list = s.split(' ')\n",
    "    token_list = [s for s in token_list if s not in stopwords_english]\n",
    "    return ' '.join(token_list)\n",
    "df['comment'] = df['comment'].apply(remove_stopwords)\n",
    "\n",
    "# Removendo números\n",
    "def remove_nums(s):\n",
    "    return ''.join([c for c in s if c not in ('1234567890')])\n",
    "df['comment'] = df['comment'].apply(remove_nums)\n",
    "\n",
    "# Stemming\n",
    "def stemming(s):\n",
    "    stemmer = PorterStemmer()\n",
    "    token_list = s.split(' ')\n",
    "    token_list = [stemmer.stem(s) for s in token_list]\n",
    "    return ' '.join(token_list)\n",
    "df['comment'] = df['comment'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "349a3a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 2515)\n",
      "ROC-AUC treino: 0.883233137829912\n",
      "ROC-AUC teste: 0.7229334597755652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "words_matrix = countvec.fit_transform(df['comment'])\n",
    "words_df = pd.DataFrame(words_matrix.todense(), columns=countvec.get_feature_names_out())\n",
    "df = pd.concat([df, words_df], axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=1312)\n",
    "model_cols = [c for c in df.columns if (c!='comment' and c!='target_sentiment')]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfr = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=10,\n",
    "                             class_weight='balanced',\n",
    "                             min_samples_split=10,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "rfr.fit(df_train[model_cols], df_train['target_sentiment'])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('ROC-AUC treino:', roc_auc_score(df_train['target_sentiment'], rfr.predict_proba(df_train[model_cols])[:,1]))\n",
    "print('ROC-AUC teste:', roc_auc_score(df_test['target_sentiment'], rfr.predict_proba(df_test[model_cols])[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249f68f",
   "metadata": {},
   "source": [
    "Já conseguimos uma melhoria no desempenho! Como podemos melhorar ainda mais nossa extração de features?\n",
    "\n",
    "Um dos jeitos é considerar _n-grams_ do texto, isto é, agrupamentos de n palavras ao invés de somente uma. Vamos reconstruir as variáveis utilizando até 3-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "585ac351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 14918)\n",
      "ROC-AUC treino: 0.8130645161290323\n",
      "ROC-AUC teste: 0.6708550655919077\n"
     ]
    }
   ],
   "source": [
    "# Recarregando o dataset\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='\t', names=['comment', 'target_sentiment'])\n",
    "# Removendo letras maiúsculas\n",
    "df['comment'] = df['comment'].apply(lambda s: s.lower())\n",
    "# Removendo pontuação\n",
    "df['comment'] = df['comment'].apply(remove_punct)\n",
    "# Removendo outros caracteres especiais\n",
    "df['comment'] = df['comment'].apply(unidecode)\n",
    "# Tratando &\n",
    "df['comment'] = df['comment'].apply(lambda s: s.replace('&', 'and'))\n",
    "\n",
    "# Removendo stopwords\n",
    "def remove_stopwords(s):\n",
    "    global stopwords_english\n",
    "    token_list = s.split(' ')\n",
    "    token_list = [s for s in token_list if s not in stopwords_english]\n",
    "    return ' '.join(token_list)\n",
    "df['comment'] = df['comment'].apply(remove_stopwords)\n",
    "\n",
    "# Removendo números\n",
    "def remove_nums(s):\n",
    "    return ''.join([c for c in s if c not in ('1234567890')])\n",
    "df['comment'] = df['comment'].apply(remove_nums)\n",
    "\n",
    "# Stemming\n",
    "def stemming(s):\n",
    "    stemmer = PorterStemmer()\n",
    "    token_list = s.split(' ')\n",
    "    token_list = [stemmer.stem(s) for s in token_list]\n",
    "    return ' '.join(token_list)\n",
    "df['comment'] = df['comment'].apply(stemming)\n",
    "\n",
    "# Vamos definir ngram_range no CountVectorizer()\n",
    "# no mínimo 1 palavra, no máximo 3\n",
    "countvec = CountVectorizer(ngram_range=(1, 3))\n",
    "words_matrix = countvec.fit_transform(df['comment'])\n",
    "words_df = pd.DataFrame(words_matrix.todense(), columns=countvec.get_feature_names_out())\n",
    "df = pd.concat([df, words_df], axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=1312)\n",
    "model_cols = [c for c in df.columns if (c!='comment' and c!='target_sentiment')]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfr = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=10,\n",
    "                             class_weight='balanced',\n",
    "                             min_samples_split=10,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "rfr.fit(df_train[model_cols], df_train['target_sentiment'])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('ROC-AUC treino:', roc_auc_score(df_train['target_sentiment'], rfr.predict_proba(df_train[model_cols])[:,1]))\n",
    "print('ROC-AUC teste:', roc_auc_score(df_test['target_sentiment'], rfr.predict_proba(df_test[model_cols])[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f1810",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Ao invés de simplesmente contarmos a frequência das palavras, podemos construir uma separação melhor utilizando o método TF-IDF - _Term Frequency-Inverse Document Frequency_. Esta técnica de vetorização consiste basicamente em dividir a frequência da palavra _na amostra_ (contagem) pela frequência da palavra _no dataset_. Dessa forma, conseguimos destacar palavras \"raras\" porém relevantes, e diminuir a importância de palavras mais corriqueiras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67cf6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 14918)\n",
      "ROC-AUC treino: 0.8633211143695014\n",
      "ROC-AUC teste: 0.6209894104630946\n"
     ]
    }
   ],
   "source": [
    "# Recarregando o dataset\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='\t', names=['comment', 'target_sentiment'])\n",
    "# Removendo letras maiúsculas\n",
    "df['comment'] = df['comment'].apply(lambda s: s.lower())\n",
    "# Removendo pontuação\n",
    "df['comment'] = df['comment'].apply(remove_punct)\n",
    "# Removendo outros caracteres especiais\n",
    "df['comment'] = df['comment'].apply(unidecode)\n",
    "# Tratando &\n",
    "df['comment'] = df['comment'].apply(lambda s: s.replace('&', 'and'))\n",
    "\n",
    "# Removendo stopwords\n",
    "def remove_stopwords(s):\n",
    "    global stopwords_english\n",
    "    token_list = s.split(' ')\n",
    "    token_list = [s for s in token_list if s not in stopwords_english]\n",
    "    return ' '.join(token_list)\n",
    "df['comment'] = df['comment'].apply(remove_stopwords)\n",
    "\n",
    "# Removendo números\n",
    "def remove_nums(s):\n",
    "    return ''.join([c for c in s if c not in ('1234567890')])\n",
    "df['comment'] = df['comment'].apply(remove_nums)\n",
    "\n",
    "# Stemming\n",
    "def stemming(s):\n",
    "    stemmer = PorterStemmer()\n",
    "    token_list = s.split(' ')\n",
    "    token_list = [stemmer.stem(s) for s in token_list]\n",
    "    return ' '.join(token_list)\n",
    "df['comment'] = df['comment'].apply(stemming)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# no mínimo 1 palavra, no máximo 3\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 3))\n",
    "words_matrix = tfidf.fit_transform(df['comment'])\n",
    "words_df = pd.DataFrame(words_matrix.todense(), columns=countvec.get_feature_names_out())\n",
    "df = pd.concat([df, words_df], axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=1312)\n",
    "model_cols = [c for c in df.columns if (c!='comment' and c!='target_sentiment')]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfr = RandomForestClassifier(n_estimators=10,\n",
    "                             max_depth=10,\n",
    "                             class_weight='balanced',\n",
    "                             min_samples_split=10,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "rfr.fit(df_train[model_cols], df_train['target_sentiment'])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('ROC-AUC treino:', roc_auc_score(df_train['target_sentiment'], rfr.predict_proba(df_train[model_cols])[:,1]))\n",
    "print('ROC-AUC teste:', roc_auc_score(df_test['target_sentiment'], rfr.predict_proba(df_test[model_cols])[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa1e9e",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "- Varie os hiperparâmetros da Random Forest ajustada e tente obter um desempenho melhor dos modelos. Execute múltiplas iterações devido à natureza aleatória dos algoritmos envolvidos.\n",
    "- Implemente o algoritmo Naïve-Bayes neste dataset e compare seu desempenho com a Random Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
